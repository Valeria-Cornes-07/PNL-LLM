{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Bases de Datos Vectoriales y Búsqueda Semántica\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "\n",
    "En este cuaderno vas a aprender:\n",
    "- Qué son los embeddings y cómo representan el significado del texto\n",
    "- Cómo funcionan las bases de datos vectoriales (ChromaDB)\n",
    "- La diferencia entre búsqueda por palabra clave y búsqueda semántica\n",
    "- Cómo construir un sistema de búsqueda inteligente en español\n",
    "- Los fundamentos de la recuperación de información para RAG\n",
    "\n",
    "## Contexto: El Problema de la Búsqueda Tradicional\n",
    "\n",
    "Imaginate que tenés una base de datos con miles de reviews de restaurantes. Un usuario busca \"lugares con buena comida italiana económica\". \n",
    "\n",
    "**Búsqueda tradicional** (por palabra clave):\n",
    "- Solo encuentra documentos que contienen exactamente las palabras \"italiana\" y \"económica\"\n",
    "- Se pierde reviews que dicen \"pasta excelente\" o \"precios accesibles\"\n",
    "\n",
    "**Búsqueda semántica** (por significado):\n",
    "- Entiende que \"pasta\" está relacionado con \"italiana\"\n",
    "- Relaciona \"precios accesibles\" con \"económica\"\n",
    "- Encuentra documentos relevantes aunque usen palabras diferentes\n",
    "\n",
    "Esto es posible gracias a los **embeddings** y las **bases de datos vectoriales**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Instalación de Dependencias\n",
    "\n",
    "Vamos a usar:\n",
    "- **chromadb**: Base de datos vectorial de código abierto\n",
    "- **sentence-transformers**: Modelos para generar embeddings de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq chromadb\n",
    "!pip install -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos que PyTorch está instalado (necesario para sentence-transformers)\n",
    "import torch\n",
    "print(f\"PyTorch versión: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## ¿Qué son los Embeddings?\n",
    "\n",
    "Un **embedding** es una representación numérica (vector) del significado de un texto. \n",
    "\n",
    "**Analogía**: Imaginate un mapa de conceptos en un espacio multidimensional:\n",
    "- Palabras con significados similares están cerca en este espacio\n",
    "- Palabras con significados diferentes están lejos\n",
    "\n",
    "Por ejemplo:\n",
    "- \"excelente\", \"genial\", \"buenísimo\" → vectores cercanos\n",
    "- \"terrible\", \"pésimo\", \"horrible\" → vectores cercanos entre sí, pero lejos de los anteriores\n",
    "\n",
    "### Ejemplo Visual\n",
    "\n",
    "```\n",
    "\"buenísimo\"     ●\n",
    "                 \\\n",
    "\"genial\"          ●     ← Estos están cerca\n",
    "                   \\\n",
    "\"excelente\"         ●\n",
    "\n",
    "\n",
    "                              ● \"terrible\"\n",
    "                             /\n",
    "                            ● \"pésimo\"     ← Estos también están cerca\n",
    "                           /\n",
    "                          ● \"horrible\"\n",
    "```\n",
    "\n",
    "**En la práctica**: Un embedding es una lista de números (típicamente 384 o 768 dimensiones) que captura el significado del texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## ChromaDB: Base de Datos Vectorial\n",
    "\n",
    "**ChromaDB** es una base de datos especializada en almacenar y buscar vectores de manera eficiente.\n",
    "\n",
    "### Características principales:\n",
    "- **Gratuita y open-source**\n",
    "- **Fácil de usar**: API simple en Python\n",
    "- **Embeddings automáticos**: Convierte texto a vectores automáticamente\n",
    "- **Búsqueda por similitud**: Encuentra documentos cercanos en el espacio vectorial\n",
    "- **Metadatos**: Permite filtrar por campos adicionales\n",
    "\n",
    "### Operaciones básicas (CRUD):\n",
    "- **Create**: `collection.add()` - Agregar documentos\n",
    "- **Read**: `collection.get()` - Obtener documentos por ID\n",
    "- **Update**: `collection.update()` - Modificar documentos\n",
    "- **Delete**: `collection.delete()` - Eliminar documentos\n",
    "- **Query**: `collection.query()` - Buscar por similitud semántica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Creamos un cliente de ChromaDB en memoria\n",
    "# Nota: Los datos se pierden al cerrar el notebook\n",
    "# Para persistencia, usar: chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "client = chromadb.Client()\n",
    "\n",
    "print(\"ChromaDB inicializado correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos que no hay colecciones todavía\n",
    "client.list_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Ejemplo Práctico: Base de Reviews de Restaurantes\n",
    "\n",
    "Vamos a crear una base de datos con reviews de restaurantes porteños. Esto nos va a permitir:\n",
    "- Buscar restaurantes por tipo de comida sin usar palabras exactas\n",
    "- Encontrar lugares similares aunque se describan diferente\n",
    "- Recomendar en base a preferencias expresadas en lenguaje natural\n",
    "\n",
    "### Pasos:\n",
    "1. Crear una colección\n",
    "2. Agregar reviews iniciales\n",
    "3. Probar búsquedas semánticas\n",
    "4. Comparar con búsqueda tradicional\n",
    "5. Mejorar con embeddings multilenguaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestra primera colección\n",
    "# Una colección es como una tabla en SQL, agrupa documentos relacionados\n",
    "collection = client.create_collection(\n",
    "    name=\"reviews_restaurantes\"\n",
    ")\n",
    "\n",
    "print(\"Colección 'reviews_restaurantes' creada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos que la colección existe\n",
    "client.list_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Dataset: Reviews de Restaurantes Porteños\n",
    "\n",
    "Vamos a usar reviews realistas de diferentes tipos de lugares en Buenos Aires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews iniciales - variedad de restaurantes en CABA\n",
    "reviews_iniciales = [\n",
    "    \"Fui a cenar pasta y la verdad que espectacular. Los ñoquis con salsa fileto increíbles. Precio razonable, como 8 lucas por persona. Ambiente tranquilo, perfecto para ir en pareja.\",\n",
    "    \n",
    "    \"La mejor pizza que comí en mi vida, no jodo. Masa finita, crocante, mucha muzzarella. Eso sí, siempre está lleno y hay que esperar. Vale la pena. Estilo porteño posta.\",\n",
    "    \n",
    "    \"Restaurante gourmet en Palermo, cocina de autor. Los platos son chicos pero re elaborados. Caro pero para una ocasión especial está bueno. Tiene buen vino también.\",\n",
    "    \n",
    "    \"Parrilla clásica de barrio. El bife de chorizo una locura, tierno y jugoso. Las papas fritas caseras. Atención familiar, te hacen sentir como en tu casa. Precios normales.\",\n",
    "    \n",
    "    \"Sushi delivery que pedimos seguido. Fresco, bien armado, llega rápido. No es el mejor que probé pero para el precio está más que bien. El combo para dos es suficiente.\"\n",
    "]\n",
    "\n",
    "# Metadatos: información adicional sobre cada review\n",
    "metadatos_iniciales = [\n",
    "    {\"barrio\": \"San Telmo\", \"tipo\": \"italiana\", \"precio\": \"medio\"},\n",
    "    {\"barrio\": \"Palermo\", \"tipo\": \"pizzeria\", \"precio\": \"medio\"},\n",
    "    {\"barrio\": \"Palermo\", \"tipo\": \"gourmet\", \"precio\": \"alto\"},\n",
    "    {\"barrio\": \"Villa Urquiza\", \"tipo\": \"parrilla\", \"precio\": \"medio\"},\n",
    "    {\"barrio\": \"delivery\", \"tipo\": \"sushi\", \"precio\": \"medio\"}\n",
    "]\n",
    "\n",
    "# IDs únicos para cada documento\n",
    "ids_iniciales = [\"review1\", \"review2\", \"review3\", \"review4\", \"review5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "### Agregando Documentos a la Colección\n",
    "\n",
    "Cuando agregamos documentos, ChromaDB automáticamente:\n",
    "1. Genera embeddings de cada texto usando un modelo por defecto\n",
    "2. Almacena los vectores en una estructura optimizada para búsqueda\n",
    "3. Guarda los metadatos asociados\n",
    "\n",
    "**Nota**: El modelo por defecto (`all-MiniLM-L6-v2`) funciona mejor con inglés. Más adelante vamos a usar un modelo multilenguaje optimizado para español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos las reviews a la colección\n",
    "collection.add(\n",
    "    documents=reviews_iniciales,\n",
    "    metadatas=metadatos_iniciales,\n",
    "    ids=ids_iniciales\n",
    ")\n",
    "\n",
    "print(f\"Se agregaron {len(reviews_iniciales)} reviews a la base de datos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos cuántos documentos tenemos\n",
    "collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos todos los documentos para verificar\n",
    "todos = collection.get()\n",
    "print(\"Documentos en la colección:\")\n",
    "for i, doc in enumerate(todos['documents'], 1):\n",
    "    print(f\"\\n{i}. {doc[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# También podemos obtener un documento específico por su ID\n",
    "review_especifica = collection.get(ids=[\"review2\"])\n",
    "print(\"Review de pizza:\")\n",
    "print(review_especifica['documents'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Búsqueda Semántica en Acción\n",
    "\n",
    "Ahora viene lo interesante: buscar documentos por significado, no por palabras exactas.\n",
    "\n",
    "### Parámetros de búsqueda:\n",
    "- **query_texts**: La consulta en lenguaje natural\n",
    "- **n_results**: Cuántos resultados queremos (los más similares)\n",
    "- **where**: Filtros opcionales por metadatos\n",
    "\n",
    "### Cómo funciona internamente:\n",
    "1. ChromaDB convierte tu consulta en un vector\n",
    "2. Calcula la distancia (similitud) entre tu consulta y todos los documentos\n",
    "3. Devuelve los N documentos más cercanos (similares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Búsqueda 1: Quiero comer algo italiano\n",
    "consulta = \"Busco un lugar para comer buena comida italiana, tipo ravioles o ñoquis\"\n",
    "\n",
    "resultados = collection.query(\n",
    "    query_texts=[consulta],\n",
    "    n_results=3  # Los 3 más similares\n",
    ")\n",
    "\n",
    "print(f\"CONSULTA: {consulta}\")\n",
    "print(\"\\nREVIEWS MÁS SIMILARES:\")\n",
    "print(\"=\" * 80)\n",
    "for i, (doc, metadata) in enumerate(zip(resultados['documents'][0], resultados['metadatas'][0]), 1):\n",
    "    print(f\"\\n{i}. {doc}\")\n",
    "    print(f\"   Barrio: {metadata['barrio']}, Tipo: {metadata['tipo']}, Precio: {metadata['precio']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### Análisis del Resultado\n",
    "\n",
    "Fijate que **no usamos la palabra \"italiana\"** en la base de datos original, pero el sistema pudo:\n",
    "1. Entender que \"ñoquis\", \"ravioles\" están relacionados con comida italiana\n",
    "2. Identificar la review que habla de \"pasta\" y \"ñoquis\"\n",
    "3. Devolver el resultado más relevante\n",
    "\n",
    "Esto es búsqueda semántica: encuentra por **significado**, no por palabra exacta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Búsqueda 2: Lugar económico para carne\n",
    "consulta2 = \"Dónde puedo ir a comer buen asado sin gastar mucha plata?\"\n",
    "\n",
    "resultados2 = collection.query(\n",
    "    query_texts=[consulta2],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "print(f\"CONSULTA: {consulta2}\")\n",
    "print(\"\\nREVIEWS MÁS SIMILARES:\")\n",
    "print(\"=\" * 80)\n",
    "for i, (doc, metadata) in enumerate(zip(resultados2['documents'][0], resultados2['metadatas'][0]), 1):\n",
    "    print(f\"\\n{i}. {doc}\")\n",
    "    print(f\"   Barrio: {metadata['barrio']}, Tipo: {metadata['tipo']}, Precio: {metadata['precio']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "### Comparación: Búsqueda Tradicional vs Semántica\n",
    "\n",
    "Veamos qué pasa si buscamos con el método tradicional (palabra clave)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Búsqueda tradicional: buscar la palabra \"asado\" exacta\n",
    "print(\"BÚSQUEDA TRADICIONAL (palabra exacta 'asado'):\")\n",
    "busqueda_tradicional = collection.get(\n",
    "    where_document={\"$contains\": \"asado\"}\n",
    ")\n",
    "\n",
    "if len(busqueda_tradicional['documents']) > 0:\n",
    "    for doc in busqueda_tradicional['documents']:\n",
    "        print(f\"- {doc}\")\n",
    "else:\n",
    "    print(\"No se encontraron resultados con la palabra 'asado'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nBÚSQUEDA SEMÁNTICA (por significado):\")\n",
    "print(\"Encontró la parrilla aunque no use la palabra 'asado' exacta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Agregando Más Documentos Dinámicamente\n",
    "\n",
    "En una aplicación real, constantemente llegan nuevas reviews. Veamos cómo agregar documentos de forma dinámica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nuevas reviews que llegan\n",
    "nuevas_reviews = [\n",
    "    \"Bar de tragos en Palermo re copado. Buenos cócteles, música en vivo los fines de semana. Se llena bastante después de las 11. Tiene terraza.\",\n",
    "    \n",
    "    \"Cafetería de especialidad, café re rico, tienen opciones veganas. Ambiente tranquilo para trabajar con la compu. WiFi gratis y enchufes.\",\n",
    "    \n",
    "    \"Bodegón español tradicional. Las croquetas y la tortilla de papa son de otro planeta. Vino de la casa riquísimo. Porteño viejo, 100 años de historia.\",\n",
    "    \n",
    "    \"Hamburguesería gourmet. Las papas con cheddar y bacon son adictivas. Burgers de 200gr, jugosas. Podes armar la tuya. Delivery hasta las 2am.\",\n",
    "    \n",
    "    \"Cantina familiar muy casera. La comida es simple pero rica, como la que hace tu nona. Milanesas gigantes. Super económico, 6 lucas comes re bien.\"\n",
    "]\n",
    "\n",
    "nuevos_metadatos = [\n",
    "    {\"barrio\": \"Palermo\", \"tipo\": \"bar\", \"precio\": \"medio-alto\"},\n",
    "    {\"barrio\": \"Colegiales\", \"tipo\": \"cafeteria\", \"precio\": \"medio\"},\n",
    "    {\"barrio\": \"Montserrat\", \"tipo\": \"española\", \"precio\": \"medio\"},\n",
    "    {\"barrio\": \"Belgrano\", \"tipo\": \"hamburguesas\", \"precio\": \"medio\"},\n",
    "    {\"barrio\": \"Boedo\", \"tipo\": \"casera\", \"precio\": \"bajo\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función helper para agregar reviews de forma organizada\n",
    "def agregar_reviews(collection, reviews, metadatos, prefijo_id=\"review\"):\n",
    "    \"\"\"\n",
    "    Agrega nuevas reviews a una colección existente.\n",
    "    \n",
    "    Parámetros:\n",
    "    -----------\n",
    "    collection : chromadb.Collection\n",
    "        La colección donde agregar los documentos\n",
    "    reviews : list\n",
    "        Lista de textos de reviews\n",
    "    metadatos : list\n",
    "        Lista de diccionarios con metadatos\n",
    "    prefijo_id : str\n",
    "        Prefijo para generar IDs únicos\n",
    "    \"\"\"\n",
    "    # Obtenemos cuántos documentos ya hay para generar IDs únicos\n",
    "    count_actual = collection.count()\n",
    "    \n",
    "    # Generamos IDs únicos\n",
    "    nuevos_ids = [f\"{prefijo_id}{count_actual + i + 1}\" for i in range(len(reviews))]\n",
    "    \n",
    "    # Agregamos a la colección\n",
    "    collection.add(\n",
    "        documents=reviews,\n",
    "        metadatas=metadatos,\n",
    "        ids=nuevos_ids\n",
    "    )\n",
    "    \n",
    "    print(f\"Se agregaron {len(reviews)} nuevas reviews\")\n",
    "    print(f\"Total de documentos en la colección: {collection.count()}\")\n",
    "    return nuevos_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos las nuevas reviews\n",
    "ids_nuevos = agregar_reviews(collection, nuevas_reviews, nuevos_metadatos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probemos una nueva búsqueda con el dataset ampliado\n",
    "consulta3 = \"Lugar tranquilo para tomar café y laburar un rato\"\n",
    "\n",
    "resultados3 = collection.query(\n",
    "    query_texts=[consulta3],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "print(f\"CONSULTA: {consulta3}\")\n",
    "print(\"\\nREVIEWS MÁS SIMILARES:\")\n",
    "print(\"=\" * 80)\n",
    "for i, (doc, metadata) in enumerate(zip(resultados3['documents'][0], resultados3['metadatas'][0]), 1):\n",
    "    print(f\"\\n{i}. {doc}\")\n",
    "    print(f\"   Barrio: {metadata['barrio']}, Tipo: {metadata['tipo']}, Precio: {metadata['precio']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Mejorando con Embeddings Multilenguaje\n",
    "\n",
    "El modelo por defecto de ChromaDB (`all-MiniLM-L6-v2`) fue entrenado principalmente con texto en inglés. Para español, especialmente con modismos argentinos, necesitamos un modelo mejor.\n",
    "\n",
    "### Modelo Recomendado: multilingual-e5-large\n",
    "\n",
    "**Características**:\n",
    "- Entrenado en 94 idiomas, incluido español\n",
    "- Entiende variantes regionales y coloquialismos\n",
    "- Mejor para textos con vocabulario local (\"posta\", \"re\", \"trucho\", etc.)\n",
    "- 768 dimensiones (vs 384 del modelo por defecto)\n",
    "\n",
    "**Trade-off**: Más lento y usa más memoria, pero mucho más preciso para español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Configuramos el modelo multilenguaje\n",
    "# Esto descarga el modelo la primera vez (puede tardar unos minutos)\n",
    "modelo_multilenguaje = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"intfloat/multilingual-e5-large\"\n",
    ")\n",
    "\n",
    "print(\"Modelo multilenguaje cargado\")\n",
    "print(\"Este modelo entiende español argentino mucho mejor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "### Creando una Nueva Colección con Mejor Modelo\n",
    "\n",
    "Vamos a crear una nueva colección usando el modelo multilenguaje y comparar los resultados.\n",
    "\n",
    "**Parámetros importantes**:\n",
    "- `embedding_function`: El modelo que convierte texto a vectores\n",
    "- `metadata={\"hnsw:space\": \"cosine\"}`: Usa similitud coseno para comparar vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una nueva colección con el modelo mejorado\n",
    "collection_mejorada = client.get_or_create_collection(\n",
    "    name=\"reviews_restaurantes_multilenguaje\",\n",
    "    embedding_function=modelo_multilenguaje,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}  # Método de comparación de vectores\n",
    ")\n",
    "\n",
    "print(\"Colección con embeddings multilenguaje creada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos las colecciones disponibles\n",
    "print(\"Colecciones en la base de datos:\")\n",
    "for col in client.list_collections():\n",
    "    print(f\"  - {col.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos TODAS las reviews (iniciales + nuevas) a la colección mejorada\n",
    "todas_las_reviews = reviews_iniciales + nuevas_reviews\n",
    "todos_los_metadatos = metadatos_iniciales + nuevos_metadatos\n",
    "todos_los_ids = ids_iniciales + ids_nuevos\n",
    "\n",
    "collection_mejorada.add(\n",
    "    documents=todas_las_reviews,\n",
    "    metadatas=todos_los_metadatos,\n",
    "    ids=todos_los_ids\n",
    ")\n",
    "\n",
    "print(f\"Se agregaron {len(todas_las_reviews)} reviews a la colección mejorada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "### Comparación: Modelo Base vs Modelo Multilenguaje\n",
    "\n",
    "Vamos a hacer la misma búsqueda con ambos modelos y comparar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consulta con modismos argentinos\n",
    "consulta_argentina = \"Busco un lugar re copado para morfar algo barato y llenadero\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"CONSULTA: {consulta_argentina}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Búsqueda con modelo base\n",
    "print(\"\\n1. MODELO BASE (all-MiniLM-L6-v2):\")\n",
    "print(\"-\"*80)\n",
    "resultado_base = collection.query(\n",
    "    query_texts=[consulta_argentina],\n",
    "    n_results=3\n",
    ")\n",
    "for i, doc in enumerate(resultado_base['documents'][0], 1):\n",
    "    print(f\"\\n{i}. {doc[:100]}...\")\n",
    "\n",
    "# Búsqueda con modelo multilenguaje\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n2. MODELO MULTILENGUAJE (multilingual-e5-large):\")\n",
    "print(\"-\"*80)\n",
    "resultado_mejorado = collection_mejorada.query(\n",
    "    query_texts=[consulta_argentina],\n",
    "    n_results=3\n",
    ")\n",
    "for i, (doc, metadata) in enumerate(zip(resultado_mejorado['documents'][0], resultado_mejorado['metadatas'][0]), 1):\n",
    "    print(f\"\\n{i}. {doc[:100]}...\")\n",
    "    print(f\"   Tipo: {metadata['tipo']}, Precio: {metadata['precio']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "### Análisis de la Comparación\n",
    "\n",
    "El modelo multilenguaje entiende mejor:\n",
    "- **\"re copado\"** → lugares con buen ambiente\n",
    "- **\"morfar\"** → comer\n",
    "- **\"barato y llenadero\"** → buena relación precio/cantidad\n",
    "\n",
    "Esto resulta en recomendaciones más relevantes para usuarios argentinos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-38",
   "metadata": {},
   "source": [
    "## Experimentación: Zona de Pruebas\n",
    "\n",
    "Probá tus propias consultas y observá cómo funciona la búsqueda semántica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TU TURNO: Modifica esta consulta\n",
    "MI_CONSULTA = \"Quiero ir a comer algo rico pero sin gastar mucho\"\n",
    "\n",
    "# Cantidad de resultados que querés ver\n",
    "CANTIDAD_RESULTADOS = 3\n",
    "\n",
    "# Ejecutamos la búsqueda\n",
    "mis_resultados = collection_mejorada.query(\n",
    "    query_texts=[MI_CONSULTA],\n",
    "    n_results=CANTIDAD_RESULTADOS\n",
    ")\n",
    "\n",
    "print(f\"CONSULTA: {MI_CONSULTA}\")\n",
    "print(\"\\nRECOMENDACIONES:\")\n",
    "print(\"=\"*80)\n",
    "for i, (doc, metadata) in enumerate(zip(mis_resultados['documents'][0], mis_resultados['metadatas'][0]), 1):\n",
    "    print(f\"\\n{i}. {doc}\")\n",
    "    print(f\"   Barrio: {metadata['barrio']}, Tipo: {metadata['tipo']}, Precio: {metadata['precio']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-40",
   "metadata": {},
   "source": [
    "### Ejercicios Sugeridos\n",
    "\n",
    "Probá estas búsquedas y observá los resultados:\n",
    "\n",
    "1. \"Un lugar romántico para llevar a mi pareja\"\n",
    "2. \"Dónde puedo comer algo rápido al mediodía\"\n",
    "3. \"Restaurante con opciones vegetarianas\"\n",
    "4. \"Lugar con buena onda para ir con amigos\"\n",
    "5. \"Comida casera como la que hace mi abuela\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-41",
   "metadata": {},
   "source": [
    "## Conexión con RAG (Próximo Paso)\n",
    "\n",
    "Lo que hicimos hoy es la **primera mitad de RAG**: la parte de **recuperación** (Retrieval).\n",
    "\n",
    "### RAG = Retrieval (Recuperar) + Augmented Generation (Generar con Contexto)\n",
    "\n",
    "**Lo que ya sabemos hacer** (este cuaderno):\n",
    "1. Almacenar documentos con sus embeddings\n",
    "2. Buscar documentos similares a una consulta\n",
    "3. Obtener los más relevantes\n",
    "\n",
    "**Lo que vamos a aprender** (próximo cuaderno):\n",
    "4. Tomar los documentos encontrados\n",
    "5. Pasárselos como contexto a un LLM (GPT/Gemini)\n",
    "6. Generar una respuesta personalizada basada en esos documentos\n",
    "\n",
    "### Ejemplo de RAG Completo\n",
    "\n",
    "**Usuario pregunta**: \"Recomendame un lugar para comer pasta buena y económica\"\n",
    "\n",
    "**Paso 1 - RETRIEVAL** (lo que ya sabemos):\n",
    "- Buscar en nuestra base vectorial\n",
    "- Encontrar: Review de la trattoria con ñoquis, review de la cantina familiar\n",
    "\n",
    "**Paso 2 - GENERATION** (próxima clase):\n",
    "- Prompt a Gemini: \"Basándote en estas reviews: [contexto], recomienda un lugar para pasta económica\"\n",
    "- Gemini genera: \"Te recomiendo la cantina familiar en Boedo. Según las reviews, la comida es casera y las porciones son generosas...\"\n",
    "\n",
    "### ¿Por qué es Poderoso RAG?\n",
    "\n",
    "1. **Información específica**: El LLM usa TUS datos, no solo su conocimiento general\n",
    "2. **Actualizable**: Agregás nuevas reviews y el sistema las usa inmediatamente\n",
    "3. **Verificable**: Podés mostrar qué documentos se usaron para generar la respuesta\n",
    "4. **Privado**: Tus datos quedan en tu base, no se envían para entrenar modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulación de cómo funcionará RAG (próxima clase)\n",
    "def simular_rag(consulta_usuario):\n",
    "    \"\"\"\n",
    "    Simulación simple de un sistema RAG completo.\n",
    "    En la próxima clase implementaremos la parte de generación con Gemini.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SIMULACIÓN DE SISTEMA RAG\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nConsulta del usuario: {consulta_usuario}\")\n",
    "    \n",
    "    # PASO 1: RETRIEVAL (ya lo sabemos hacer)\n",
    "    print(\"\\n[PASO 1] RETRIEVAL - Buscando documentos relevantes...\")\n",
    "    resultados = collection_mejorada.query(\n",
    "        query_texts=[consulta_usuario],\n",
    "        n_results=2\n",
    "    )\n",
    "    \n",
    "    docs_relevantes = resultados['documents'][0]\n",
    "    print(f\"Se encontraron {len(docs_relevantes)} documentos relevantes:\")\n",
    "    for i, doc in enumerate(docs_relevantes, 1):\n",
    "        print(f\"  {i}. {doc[:80]}...\")\n",
    "    \n",
    "    # PASO 2: GENERATION (próxima clase con Gemini)\n",
    "    print(\"\\n[PASO 2] GENERATION - Generando respuesta personalizada...\")\n",
    "    print(\"(Próxima clase: usaremos Gemini/GPT con este contexto)\")\n",
    "    print(\"\\nRespuesta simulada:\")\n",
    "    print(\"Basándome en las reviews encontradas, te recomiendo...\")\n",
    "    print(\"[Aquí Gemini generaría una respuesta natural usando los documentos]\")\n",
    "    \n",
    "    return docs_relevantes\n",
    "\n",
    "# Probemos la simulación\n",
    "simular_rag(\"Quiero un lugar barato para comer mucho\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-43",
   "metadata": {},
   "source": [
    "## Resumen y Conceptos Clave\n",
    "\n",
    "### Lo que aprendimos:\n",
    "\n",
    "1. **Embeddings**: Representaciones numéricas del significado del texto\n",
    "   - Palabras similares tienen vectores similares\n",
    "   - Permiten comparar textos por significado, no solo por palabras\n",
    "\n",
    "2. **ChromaDB**: Base de datos vectorial\n",
    "   - Almacena embeddings de forma eficiente\n",
    "   - Operaciones CRUD estándar\n",
    "   - Búsqueda por similitud semántica\n",
    "\n",
    "3. **Búsqueda Semántica vs Tradicional**:\n",
    "   - Tradicional: Solo encuentra palabras exactas\n",
    "   - Semántica: Entiende sinónimos, contexto, significado\n",
    "\n",
    "4. **Modelos de Embeddings**:\n",
    "   - `all-MiniLM-L6-v2`: Rápido, bueno para inglés\n",
    "   - `multilingual-e5-large`: Mejor para español, modismos, regionalismos\n",
    "\n",
    "5. **Parámetros Configurables**:\n",
    "   - `n_results`: Cantidad de documentos a devolver\n",
    "   - `embedding_function`: Modelo que genera los vectores\n",
    "   - `metadata`: Información adicional para filtrar\n",
    "\n",
    "### Próximos pasos:\n",
    "\n",
    "En el siguiente cuaderno vamos a:\n",
    "- Integrar ChromaDB con Gemini/GPT\n",
    "- Construir un sistema RAG completo\n",
    "- Generar respuestas contextualizadas basadas en nuestros documentos\n",
    "- Crear un asistente que responde usando información específica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-44",
   "metadata": {},
   "source": [
    "## Glosario\n",
    "\n",
    "**Embedding**: Vector numérico que representa el significado de un texto. Textos con significados similares tienen embeddings similares.\n",
    "\n",
    "**Vector**: Lista de números (típicamente 384 o 768 valores) que representa un punto en un espacio multidimensional.\n",
    "\n",
    "**Base de Datos Vectorial**: Sistema especializado en almacenar y buscar vectores de forma eficiente (ej: ChromaDB, Pinecone, Weaviate).\n",
    "\n",
    "**Similitud Coseno**: Medida de qué tan similar es la dirección de dos vectores. Valor entre -1 (opuestos) y 1 (idénticos).\n",
    "\n",
    "**Búsqueda Semántica**: Búsqueda por significado en lugar de coincidencia exacta de palabras.\n",
    "\n",
    "**Colección**: Grupo de documentos relacionados en ChromaDB, similar a una tabla en SQL.\n",
    "\n",
    "**Metadatos**: Información adicional sobre un documento (ej: fecha, autor, categoría) que no es parte del texto principal.\n",
    "\n",
    "**RAG (Retrieval Augmented Generation)**: Técnica que combina búsqueda de información relevante con generación de texto usando LLMs.\n",
    "\n",
    "**Query**: Consulta o pregunta que se hace a la base de datos.\n",
    "\n",
    "**HNSW (Hierarchical Navigable Small World)**: Algoritmo eficiente para búsqueda de vecinos más cercanos en espacios de alta dimensión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-45",
   "metadata": {},
   "source": [
    "## Preguntas Frecuentes\n",
    "\n",
    "**P: ¿Por qué usar una base de datos vectorial en lugar de una base de datos normal?**\n",
    "\n",
    "R: Las bases vectoriales están optimizadas para buscar por similitud semántica. Una SQL busca coincidencias exactas; una vectorial encuentra \"lo más parecido\" incluso si las palabras son diferentes.\n",
    "\n",
    "---\n",
    "\n",
    "**P: ¿Cuánto espacio ocupan los embeddings?**\n",
    "\n",
    "R: Depende del modelo. multilingual-e5-large usa 768 dimensiones × 4 bytes = ~3KB por documento. Para 1 millón de documentos serían ~3GB.\n",
    "\n",
    "---\n",
    "\n",
    "**P: ¿Puedo usar ChromaDB en producción?**\n",
    "\n",
    "R: Sí, pero para aplicaciones grandes considerá alternativas como Pinecone o Weaviate que están más optimizadas para escala. ChromaDB es excelente para prototipos y aplicaciones medianas.\n",
    "\n",
    "---\n",
    "\n",
    "**P: ¿Qué pasa si mi texto tiene más de 512 tokens?**\n",
    "\n",
    "R: Los modelos de embeddings tienen un límite (típicamente 512 tokens). Si tu documento es más largo, tenés que dividirlo en chunks. Lo vemos en el próximo cuaderno.\n",
    "\n",
    "---\n",
    "\n",
    "**P: ¿Los embeddings se actualizan cuando cambio un documento?**\n",
    "\n",
    "R: No, los embeddings son estáticos. Si modificás un documento, tenés que regenerar su embedding usando `collection.update()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-46",
   "metadata": {},
   "source": [
    "## Referencias y Recursos\n",
    "\n",
    "**Documentación**:\n",
    "- ChromaDB Docs: https://docs.trychroma.com/\n",
    "- Sentence Transformers: https://www.sbert.net/\n",
    "- Modelo multilingual-e5: https://huggingface.co/intfloat/multilingual-e5-large\n",
    "\n",
    "**Papers Relevantes**:\n",
    "- \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\"\n",
    "- \"Text Embeddings by Weakly-Supervised Contrastive Pre-training\" (E5)\n",
    "- \"Efficient and Robust Approximate Nearest Neighbor Search Using HNSW\"\n",
    "\n",
    "**Recursos Adicionales**:\n",
    "- Lista de modelos de embeddings: https://huggingface.co/spaces/mteb/leaderboard\n",
    "- Tutorial interactivo de embeddings: https://projector.tensorflow.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
